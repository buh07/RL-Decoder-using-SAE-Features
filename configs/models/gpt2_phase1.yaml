model_name: gpt2
model_source: /scratch2/f004ndc/LLM Second-Order Effects/models/models--gpt2
params_million: 117
n_layers: 12
n_heads: 12
d_model: 768
vocab_size: 50257
layer_probe_default: 6  # mid-layer tap for SAE/logit-lens
context_length: 1024
tokenizer_files:
  - tokenizer.json
  - merges.txt
  - vocab.json
requirements:
  pytorch: ">=2.0"
  transformers: ">=4.37"
  trl: ">=0.8"
  accelerate: ">=0.25"
notes:
  - freeze base weights; use eval() for hidden-state dumps
  - keep tokenizer + merges pinned to repo by copying during setup_env.sh

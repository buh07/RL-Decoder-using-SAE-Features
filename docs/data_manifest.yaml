# Phase 1 corpus targets for RL-Decoder
# Token counts are approximate and assume post-tokenization with GPT-2 vocab.
supervised_train:
  description: "100M-token clean corpus for decoder pre-training"
  target_tokens: 100_000_000
  shards:
    - name: wikitext-103
      tokens: 45_000_000
      license: CC-BY-SA-3.0
      notes: "high quality encyclopedic text; use train split only"
    - name: openwebtext_10k
      tokens: 30_000_000
      license: PDDL
      notes: "filtered web pages; remove boilerplate + PII"
    - name: stackexchange_math
      tokens: 15_000_000
      license: CC-BY-SA-4.0
      notes: "math/bool reasoning; strip code blocks for now"
    - name: synthetic_task_prompts
      tokens: 10_000_000
      license: internal
      notes: "hand-built reasoning traces for curriculum warm-start"
rl_buffer:
  description: "10B-token buffer for PPO rollouts"
  target_tokens: 10_000_000_000
  composition:
    instructional_mix:
      share: 0.35
      datasets: [self_instruct_math, gsm8k_augmented, boolq_augmented]
    reflective_mix:
      share: 0.25
      datasets: [cot_reflections, curated_stackexchange_histories]
    open_domain_mix:
      share: 0.40
      datasets: [openwebtext_large, mathstack_full, custom_wikipedia_stream]
  storage:
    path: data/shards/rl_buffer
    format: torch.save({"input_ids": tensor})
evaluation_sets:
  clean_eval:
    tokens: 100_000
    datasets: [wikitext-103-validation, gsm8k-test-trimmed]
  ood_eval:
    tokens: 50_000
    datasets: [lila_math_ood, adversarial_boolq]
provenance_logging:
  location: docs/logbook.md
  required_fields:
    - dataset_name
    - source_url_or_path
    - license
    - checksum
    - preprocessing_notes

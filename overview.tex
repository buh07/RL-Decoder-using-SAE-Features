\documentclass{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{natbib}

\title{Mechanistic Interpretability of Reasoning in LLMs via Sparse Autoencoders: A Phased, Resource-Efficient, Falsifiable Framework}
\author{Benjamin Huh}
\date{February 2026}

\begin{document}

\maketitle

\begin{abstract}
This document outlines a comprehensive, phased framework to evaluate sparse autoencoders (SAEs) for extracting human-interpretable, causally verifiable reasoning features from large language model (LLM) activations. Building on SAE successes~\citep{AnthropicMappingMind} and limitations~\citep{SAE-Identifiability,SAE-Random,SAE-Empirical}, we progress from ground-truth systems to LLMs, with strict falsification criteria. Key innovations include decorrelation losses, probe-guided training with leakage diagnostics, staged alignment, and causal attribution protocols. Resource optimizations enable execution on single high-end GPUs (e.g., RTX 6000). Full details cover theory, architecture, datasets, workflow, validation, risks, and scaling.
\end{abstract}

\section{Introduction and Motivation}
LLMs excel on reasoning benchmarks, yet internal multi-step processes (e.g., hypothesis formation, constraint propagation) remain opaque. Chain-of-thought (CoT) is unreliable~\citep{Shu2025SurveySAE}. SAEs address this by learning sparse, monosemantic latents via dictionary learning~\citep{AnthropicMappingMind,ScalingMonosemanticityBlog}.

\textbf{Research Question:} Can SAEs recover stable, causal reasoning primitives across phases, or do they reflect artifacts?

Phased to falsify: Failures (e.g., no causal alignment) halt progression~\citep{SAE-Identifiability,SAE-Random}.

\section{Theory and Background}
\subsection{Sparse Autoencoders}
SAE: Encoder $f$, decoder $g$ s.t. $X \approx g(f(X))$, $L = \|X - \hat{X}\|_2^2 + \lambda \|h\|_1 + R(h)$, $h = f(X)$~\citep{Shu2025SurveySAE}. Overcomplete bases yield monosemanticity, outperforming neurons~\citep{AnthropicMappingMind,SAE-Monosemantic}.

Limitations: Non-identifiable without sparsity~\citep{SAE-Identifiability}; random nets interpretable~\citep{SAE-Random}; domain-sensitive~\citep{SAE-Domain,SAE-Empirical}.

\subsection{Monosemanticity}
Features activate consistently for one concept; polysemy fragments axes~\citep{Zhu2025AbsTopK}.

\section{Experimental Formulation}
\subsection{Activations and Hooks}
Post-MLP residual + MLP hidden (nonlinear updates). Expand to mid-late layers if needed. Token-level from reasoning datasets.

Efficiency: On-the-fly streaming, fp16, 1-2 layers, small models (1B-7B).

\subsection{SAE Architecture}
Overcomplete latent ($k=4$-$8 \times d$), ReLU. Decoder reconstructs $X$.

\subsection{Loss Function}
\[
L_\text{total} = \|X - \hat{X}\|_2^2 + \lambda \|h\|_1 + \beta \sum_{i<j} w_i^\top w_j + \gamma L_\text{probe} + \delta L_\text{temporal}.
\]
Decorrelation: Penalizes redundancy (novel directions, shared subspaces OK). Temporal optional: $\|h_t - h_{t+1}\|_2^2$. Probes: Hypothesis/constraint tasks; ablation if gap $>$5\%.

\section{Datasets and Tasks}
Structured reasoning: Multi-step math, logic, Sudoku. Corpora: GSM8K, CoT-Collection, OpenR1-Math-220k, Reasoning Traces, REVEAL, TRIP, WIQA. Subsample for efficiency.

\section{Phased Framework}
\begin{enumerate}
\item \textbf{Ground-Truth (Phase 1)}: BFS/DFS, stacks; align SAE to states~\citep{SAE-Monosemantic}.
\item \textbf{Synthetic Transformers (Phase 2)}: Grid/logic; causal perturbations.
\item \textbf{Controlled CoT (Phase 3)}: Labeled steps; probes; 100\% accuracy validation.
\item \textbf{Frontier LLMs (Phase 4)}: Multi-model analysis (GPT-2-medium, Pythia-1.4B, Gemma-2B, Phi-2); feature sparsity metrics.
\item \textbf{Interpretability Validation (Phase 4B)}: Feature purity, selectivity analysis, semantic descriptions.
\item \textbf{Causal Attribution (Phase 5)}: Validate importance estimates; rank features by true causal impact; identify model-architecture dependencies.
\item \textbf{CoT-SAE Extension (Phase 6, proposed)}: Step-level causal testing; reasoning controllability; cross-model universality.
\end{enumerate}

\section{Experimental Workflow}
\begin{enumerate}
\item Capture (stream, monitor).
\item Train Phase 1: Recon + sparsity.
\item Phase 2: +Decorr/probes.
\item Align: Multi-heuristic (regex, similarity), confidence-weighted; EM refine.
\item Inspect: LM descriptions, clustering.
\item Validate: Purity, causal ($h \leftarrow h + \epsilon e_i$).
\item Iterate.
\end{enumerate}

\section{Interpretability Protocols}
\subsection{Alignment}
Step-to-token: Rules/edit-distance; multi-aligner consensus; spot-checks.

\subsection{Causal Attribution}
Baselines (recon, nulls), linearity, hook sweeps, calibration. Stats sig. required~\citep{ScalingMonosemanticityBlog}.

\subsection{Validation}
Immediate: Purity (silhouette), clusters. Post: Ablations, replication, holdouts, audits.

\section{Evaluation Metrics}
- Purity: Top-k coherence, sparsity (target $\approx 30\%$).
- Selectivity: Top-5 activation variance (monosemanticity proxy).
- Causal: Task/probe shifts; feature ablation correlation ($r > 0.7$ target).
- Probe accuracy: Step prediction on CoT sequences.
- Cross-layer: Multi-model consistency.
- Transfer: Feature utility across models.

\section{Risks and Mitigations}
- Coverage: Phased scaling~\citep{SAE-Empirical}.
- Brittleness: Invariance tests~\citep{Li2025InterpretabilityIllusions}.
- Leakage: Ablations~\citep{SAE-Identifiability}.

\section{Resources (Optimized)}
50-100 RTX-equivalent hours, $<$100 GB; 1-2 weeks single GPU.

\section{Key Findings and Insights}
\subsection{Phase 4B: Feature Monosemanticity}
Consistent $31.7\%$ sparsity across all models (GPT-2, Pythia, Gemma, Phi-2) validates genuine monosemanticity, not architecture-specific artifact.

\subsection{Phase 5: Selectivity $\neq$ Importance (Model-Dependent)}
\textbf{Critical Finding:} Feature selectivity (top-5 activation variance) correlates with task importance only for certain architectures:
\begin{itemize}
  \item Phi-2 (2.7B, reasoning-optimized): $r = 0.754$ (strong coupling)
  \item Pythia-1.4B (general): $r = 0.601$ (moderate)
  \item GPT-2-medium (125M, older): $r = 0.161$ (weak)
  \item Gemma-2B (modern compression): $r = -0.177$ (inverse)
\end{itemize}

\textbf{Implication:} Interpretable features (sparse) do not guarantee task relevance. Smaller/specialized models use selective representations; larger/general models use distributed primitives. This explains why interpretability breakthroughs often fail to impact performance.

\subsection{Phase 5.2: Semantic Clarity Achieved}
40 top features successfully named with intuitive descriptions matching activation patterns, demonstrating semantic interpretability independent of causal importance.

\section{Extensions and Future Work}
\subsection{Phase 5.3: Feature Transfer Analysis (COMPLETE)}
\textbf{Completed:} Test whether top features transfer across models (universality hypothesis); measure reconstruction quality and feature utility on alternate models.

\textbf{Key Finding:} Near-perfect transfer achieved ($0.995$-$1.000$ reconstruction ratio across model pairs). Features represent model-invariant semantic primitives, not model-specific artifacts. Three pairwise models tested:
\begin{itemize}
  \item GPT-2-medium $\to$ Pythia-1.4B: $0.998$ reconstruction, $0.945$ variance ratio
  \item GPT-2-medium $\to$ Gemma-2B: $0.998$ reconstruction, $0.891$ variance ratio
  \item Pythia-1.4B $\to$ Gemma-2B: $0.996$ reconstruction, $0.932$ variance ratio
\end{itemize}
Low decoder similarity ($0.088$ consistent) indicates distributed encodings differ while semantic content transfers perfectly. \textbf{Implication:} Models solve reasoning identically but encode solutions differently; SAE-based universality hypothesis validated.

\subsection{Phase 5.4: Multi-Layer Feature Transfer Analysis (IN PROGRESS)}
\textbf{Motivation:} Phase 5.3 examined only final-layer representations. Reasoning develops through network depths; different layers may show universal vs. specialized features. Phase 6 causal control requires layer-wise understanding.

\textbf{Approach:} Extend transfer analysis to mid-layer activations. Capture layer $\{4, 8, 12, 16, 20\}$ per model (4 models $\times$ 5 layers = 20 SAEs). Compute pairwise transfer for all combinations (400 transfer metrics). Generate layer universality heatmap.

\textbf{Research Questions:}
\begin{enumerate}
  \item Are early layers (4, 8, 12) universal across models?
  \item At what depth do models specialize?
  \item Bottleneck effect: Do models converge at certain layers?
  \item How do transfer metrics vary with network depth?
\end{enumerate}

\textbf{Expected Outcome:} Layer-wise universality map, identifying WHERE models diverge and optimal layers for Phase 6 feature steering. Integrated into Phase 6 design (layer-informed intervention strategies).

\subsection{Phase 6: Controllable Chain-of-Thought (Proposed)}
Extend to step-level causal testing: (1) Capture activations aligned with reasoning steps; (2) Test which features disrupt specific reasoning operations; (3) Measure step-wise importance (not global); (4) Steer reasoning via latent manipulation; (5) Identify universal reasoning primitives across architectures. 

\textbf{Phase 5.4 Integration:} Use multi-layer findings to target optimal layers for feature intervention, improving steering precision and interpretability.

Expected outcome: Deterministic control over multi-step reasoning without fine-tuning.

\subsection{Validation Criteria}

\textbf{Phase 5.4 Success Metrics:}
\begin{itemize}
  \item Multi-layer capture: All 20 layer activation files successfully generated
  \item SAE convergence: All 20 SAEs train to convergence (final loss $< 0.5$)
  \item Transfer matrix complete: 400 pairwise transfer metrics computed (no failures)
  \item Layer universality visible: Heatmap shows degradation pattern (validates architecture effects)
  \item No structural errors: All JSON outputs valid, no NaN/Inf values
\end{itemize}

\textbf{Phase 6 Validation Criteria:}
- Step-level feature importance: $r > 0.7$ between latent ablation and step quality
- Cross-model universality: Same features predict same steps (>70\% agreement)
- Steering efficacy: Controlled adjustment of reasoning length/quality via latent thresholds
- Generalization: Apply steering from one model, validate on another

\section{Conclusion}
This framework advances SAE interpretability for reasoning, with falsifiable phases and efficiencies. Phases 1-5 Tasks 1-3 establish that SAEs extract interpretable features (31.7\% sparsity, semantic clarity) with near-perfect universality (0.995-1.000 transfer ratio), validating model-invariant reasoning hypothesis. Phase 5.4 (multi-layer analysis in progress) will map WHERE universality breaks down with network depth. Phase 6 extension toward controllable reasoning offers practical leverage for model alignment and mechanistic understanding, informed by Phase 5.4 layer-wise findings.

\bibliographystyle{plainnat}
\begin{thebibliography}{9}

\bibitem[Anthropic Mapping the Mind(2023)]{AnthropicMappingMind}
Anthropic.
\newblock Towards Monosemanticity: Decomposing Language Models With Dictionary Learning.
\newblock \url{https://transformer-circuits.pub/2023/monosemantic-features}, 2023.

\bibitem[Anon(2025a)]{SAE-Identifiability}
Anon.
\newblock Identifiable Sparse Autoencoders: Theory \& Methods.
\newblock arXiv:2506.15963, 2025.
\url{https://arxiv.org/abs/2506.15963}.

\bibitem[Anon(2025b)]{SAE-Random}
Anon.
\newblock Sparse Autoencoders Can Interpret Randomly Initialized Transformers.
\newblock arXiv:2501.17727, 2025.
\url{https://arxiv.org/abs/2501.17727}.

\bibitem[Anon(2025c)]{SAE-Domain}
Anon.
\newblock Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders.
\newblock arXiv:2508.09363, 2025.

\bibitem[Anon(2024)]{SAE-Empirical}
Anon.
\newblock Evaluating Sparse Autoencoders for LLM Interpretability.
\newblock arXiv:2405.08366, 2024.
\url{https://arxiv.org/abs/2405.08366}.

\bibitem[Anon(2025d)]{SAE-Monosemantic}
Anon.
\newblock Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models.
\newblock arXiv:2504.02821, 2025.
\url{https://arxiv.org/abs/2504.02821}.

\bibitem[Anthropic(2024)]{ScalingMonosemanticityBlog}
Anthropic.
\newblock Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet.
\newblock \url{https://transformer-circuits.pub/2024/scaling-monosemanticity/}, 2024.

\bibitem[Shu et~al.(2025)]{Shu2025SurveySAE}
Shu~et~al.
\newblock Sparse Autoencoders Survey in Mechanistic Interpretability.
\newblock 2025.

\bibitem[Li(2025)]{Li2025InterpretabilityIllusions}
Li.
\newblock Interpretability Illusions.
\newblock 2025.

\bibitem[Zhu(2025)]{Zhu2025AbsTopK}
Zhu~et~al.
\newblock AbsTopK and SAE Limitations.
\newblock 2025.

\end{thebibliography}

\end{document}
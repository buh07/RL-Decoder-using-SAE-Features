{
  "model_name": "gpt2-medium",
  "layer_idx": 10,
  "input_dim": 1024,
  "expansion_factor": 8,
  "final_loss": 58.47046392104205,
  "min_loss": 58.47046392104205,
  "final_sparsity": 0.49987129604115205,
  "avg_sparsity": 0.49987129604115205,
  "config": {
    "input_dim": 1024,
    "expansion_factor": 8,
    "use_relu": false,
    "decoder_init_scale": 0.1,
    "l1_penalty_coeff": 0.0001,
    "reconstruction_coeff": 1.0,
    "decorrelation_coeff": 0.01,
    "probe_loss_coeff": 0.05,
    "temporal_smoothness_coeff": 0.01,
    "learning_rate": 0.0001,
    "batch_size": 64,
    "max_epochs": 10,
    "warmup_steps": 1000,
    "grad_clip": 1.0,
    "use_amp": true,
    "decoder_norm_every": 100,
    "log_every": 100,
    "checkpoint_every": 500,
    "eval_every": 500,
    "use_probes": false,
    "probe_layer_indices": [],
    "probe_task": "hypothesis",
    "probe_leakage_threshold": 0.05,
    "use_temporal": false,
    "device": "cuda",
    "dtype": "float16",
    "wandb_project": "rl-decoder-sae",
    "wandb_entity": null,
    "checkpoint_dir": 
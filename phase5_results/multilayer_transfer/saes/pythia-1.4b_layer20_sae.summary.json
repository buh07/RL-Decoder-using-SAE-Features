{
  "model_name": "pythia-1.4b",
  "layer_idx": 20,
  "input_dim": 2048,
  "expansion_factor": 8,
  "final_loss": 0.20633148358148687,
  "min_loss": 0.20633148358148687,
  "final_sparsity": 0.4999253890093635,
  "avg_sparsity": 0.49991531933055205,
  "config": {
    "input_dim": 2048,
    "expansion_factor": 8,
    "use_relu": false,
    "decoder_init_scale": 0.1,
    "l1_penalty_coeff": 5e-05,
    "reconstruction_coeff": 1.0,
    "decorrelation_coeff": 0.01,
    "probe_loss_coeff": 0.05,
    "temporal_smoothness_coeff": 0.01,
    "learning_rate": 0.0001,
    "batch_size": 64,
    "max_epochs": 10,
    "warmup_steps": 1000,
    "grad_clip": 1.0,
    "use_amp": true,
    "decoder_norm_every": 100,
    "log_every": 100,
    "checkpoint_every": 500,
    "eval_every": 500,
    "use_probes": false,
    "probe_layer_indices": [],
    "probe_task": "hypothesis",
    "probe_leakage_threshold": 0.05,
    "use_temporal": false,
    "device": "cuda",
    "dtype": "float16",
    "wandb_project": "rl-decoder-sae",
    "wandb_entity": null,
    "checkpoint_dir": "checkpoints/gpt2-small/sae",
    "save_config_to_wandb": true
  }
}